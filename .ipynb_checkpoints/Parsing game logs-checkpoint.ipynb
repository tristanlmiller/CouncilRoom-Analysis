{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing game logs\n",
    "\n",
    "Tristan Miller, 4/28/2018\n",
    "\n",
    "The goal here is to use game logs to determine which cards have the most impact.  Impact is measured by how much the presence of a card changes the players' gains. Only gains that are in the supply count (so no prizes, spoils).\n",
    "\n",
    "The data set is 140,000 game logs in which at least one of the players was in the top 100.  The game logs date back to Guilds, which makes some things easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#packages\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist = os.listdir('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#special cases for ruins and knights\n",
    "ruins = ['Ruined Market','Ruined Village','Abandoned Mine','Survivors','Ruined Library']\n",
    "knights = ['Dame Josephine','Dame Sylvia','Dame Natalie','Dame Anna','Dame Molly','Sir Martin','Sir Destry','Sir Bailey','Sir Michael','Sir Vander']\n",
    "special_cases = dict.fromkeys(knights,'Knights')\n",
    "for ruin in ruins:\n",
    "    special_cases[ruin] = 'Ruins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parse a log, extracting the cards in the supply, and the number of cards gained by each player\n",
    "def parse_log(filepath):\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        #first, get the supply cards\n",
    "        counter = 0\n",
    "        while( counter < 20 ):\n",
    "            counter += 1\n",
    "            line = f.readline()\n",
    "            m = re.match('Supply cards: (.+)$',line)\n",
    "            if( m ):\n",
    "                supply = re.split(', ',m.group(1))\n",
    "                break\n",
    "             \n",
    "        #get names of players\n",
    "        while( counter < 20 ):\n",
    "            counter += 1\n",
    "            line = f.readline()\n",
    "            m = re.match('(.+) - starting cards:',line)\n",
    "            if( m ):\n",
    "                p1name = re.escape(m.group(1))\n",
    "                break\n",
    "                \n",
    "        while( counter < 20 ):\n",
    "            counter += 1\n",
    "            line = f.readline()\n",
    "            m = re.match('(.+) - starting cards:',line)\n",
    "            if( m ):\n",
    "                p2name = re.escape(m.group(1))\n",
    "                break\n",
    "                \n",
    "        if counter == 20:\n",
    "            #If the game hasn't found the header yet, something went wrong\n",
    "            print('Error: could not parse header in',filepath)\n",
    "            raise\n",
    "        \n",
    "        #Next, calculate the number of times each card was gained\n",
    "        p1gains = dict.fromkeys(supply,0)\n",
    "        p2gains = dict.fromkeys(supply,0)\n",
    "        for line in f:\n",
    "            m = re.match('(' + p1name + '|' + p2name + ') - gains (.+)$',line)\n",
    "            if( m ):\n",
    "                if m.group(1) == p1name:\n",
    "                    gains = p1gains\n",
    "                else:\n",
    "                    gains = p2gains\n",
    "                if m.group(2) in gains:\n",
    "                    gains[m.group(2)] += 1\n",
    "                elif m.group(2) in special_cases and special_cases[m.group(2)] in gains:\n",
    "                    gains[special_cases[m.group(2)]] += 1\n",
    "    return supply, p1gains, p2gains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads supply only\n",
    "def parse_supply(filepath):\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        #first, get the supply cards\n",
    "        while( True ):\n",
    "            line = f.readline()\n",
    "            m = re.match('Supply cards: (.+)$',line)\n",
    "            if( m ):\n",
    "                supply = re.split(', ',m.group(1))\n",
    "                break\n",
    "    return supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print the log (for testing)\n",
    "def print_log(filenum):\n",
    "    with open('logs/'+filelist[filenum],'r',encoding='utf-8') as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of all supply cards\n",
    "#card_list is a list of all unique cards\n",
    "#card_dict is a dict of all unique cards, with the value being the position in card_list\n",
    "\n",
    "card_dict = {}\n",
    "for i,filename in zip(range(2000),filelist[-2000:]):\n",
    "    try:\n",
    "        supply = parse_supply('logs/'+filename)\n",
    "        for card in supply:\n",
    "            card_dict[card] = 0\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "card_list = ['']*len(card_dict)\n",
    "for i,card in enumerate(card_dict.keys()):\n",
    "    card_dict[card] = i\n",
    "    card_list[i] = card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n"
     ]
    }
   ],
   "source": [
    "print(len(card_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parse the logs, and combine data\n",
    "#num_files is the number of files to read (by default all of them)\n",
    "#num_games is an ndarray of the number of games with each pair of cards\n",
    "#game_gains is an ndarray of the number of games where [col] was gained, and had [row] in the kingdom\n",
    "#total_gains is an ndarray of the number of gains of [col] in kingdoms that have [row]\n",
    "#note that games are double counted, since we look at it from the point of view of each player\n",
    "def combine_logs(first_file = 0, last_file = -1,verbose=False):\n",
    "    start_time = time.time()\n",
    "    num_games = np.zeros((len(card_list),len(card_list)),dtype='int')\n",
    "    game_gains = np.zeros((len(card_list),len(card_list)),dtype='int') \n",
    "    total_gains = np.zeros((len(card_list),len(card_list)),dtype='int')\n",
    "    \n",
    "    if last_file < 0:\n",
    "        last_file = len(filelist)\n",
    "    \n",
    "    for i,filename in zip(range(first_file,last_file),filelist[first_file:last_file]):\n",
    "        try:\n",
    "            supply, p1gains, p2gains = parse_log('logs/'+filename)\n",
    "            for card1 in supply:\n",
    "                for card2 in supply:\n",
    "                    p = card_dict[card1]\n",
    "                    q = card_dict[card2]\n",
    "                    num_games[p][q] += 2\n",
    "                    total_gains[p][q] += p1gains[card2] + p2gains[card2]\n",
    "                    game_gains[p][q] += (1 if p1gains[card2] > 0 else 0) + (1 if p2gains[card2] > 0 else 0)\n",
    "        except:\n",
    "            print('error in file',filename)\n",
    "        if i % 1000 == 0:\n",
    "            print(i,'files processed in',(time.time() - start_time)/60,'minutes')\n",
    "                \n",
    "    return num_games, game_gains, total_gains\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files processed in 0.0011833389600118002 minutes\n",
      "1000 files processed in 0.35548386971155804 minutes\n",
      "2000 files processed in 0.7260510842005412 minutes\n",
      "3000 files processed in 1.2062184890111287 minutes\n",
      "4000 files processed in 1.5665857076644898 minutes\n",
      "5000 files processed in 1.930069589614868 minutes\n",
      "6000 files processed in 2.3793583552042645 minutes\n",
      "7000 files processed in 2.8077518343925476 minutes\n",
      "8000 files processed in 3.1695357163747153 minutes\n",
      "9000 files processed in 3.518402910232544 minutes\n",
      "10000 files processed in 3.8773034532864887 minutes\n",
      "11000 files processed in 4.257620708147685 minutes\n",
      "12000 files processed in 4.613687924544016 minutes\n",
      "13000 files processed in 4.971588456630707 minutes\n",
      "14000 files processed in 5.341205664475759 minutes\n",
      "15000 files processed in 5.687772858142853 minutes\n",
      "16000 files processed in 6.035590056578318 minutes\n",
      "17000 files processed in 6.38902390797933 minutes\n",
      "18000 files processed in 6.748841126759847 minutes\n",
      "19000 files processed in 7.119975022474924 minutes\n",
      "20000 files processed in 7.653159161408742 minutes\n",
      "21000 files processed in 8.015526362260182 minutes\n",
      "22000 files processed in 8.969411118825276 minutes\n",
      "23000 files processed in 10.189812970161437 minutes\n",
      "24000 files processed in 11.791370348135631 minutes\n",
      "25000 files processed in 12.944938711325328 minutes\n",
      "26000 files processed in 14.196323970953623 minutes\n",
      "27000 files processed in 15.214809139569601 minutes\n",
      "28000 files processed in 16.193798716862997 minutes\n",
      "29000 files processed in 17.26288369099299 minutes\n",
      "30000 files processed in 18.221335180600484 minutes\n",
      "31000 files processed in 18.822586091359458 minutes\n",
      "32000 files processed in 19.63923740784327 minutes\n",
      "33000 files processed in 20.319588466485342 minutes\n",
      "34000 files processed in 21.082509712378183 minutes\n",
      "35000 files processed in 21.450710992018383 minutes\n",
      "36000 files processed in 21.924665168921152 minutes\n",
      "37000 files processed in 22.301219713687896 minutes\n",
      "38000 files processed in 22.670570886135103 minutes\n",
      "39000 files processed in 23.041191228230794 minutes\n",
      "40000 files processed in 23.538894855976103 minutes\n",
      "41000 files processed in 23.936479616165162 minutes\n",
      "42000 files processed in 24.306047546863557 minutes\n",
      "43000 files processed in 24.659815283616386 minutes\n",
      "44000 files processed in 25.01878306468328 minutes\n",
      "45000 files processed in 25.39230097134908 minutes\n",
      "46000 files processed in 25.751402111848197 minutes\n",
      "47000 files processed in 26.090269887447356 minutes\n",
      "48000 files processed in 26.44003765185674 minutes\n",
      "49000 files processed in 27.024122746785483 minutes\n",
      "50000 files processed in 27.39297397136688 minutes\n",
      "51000 files processed in 27.78800846338272 minutes\n",
      "52000 files processed in 28.196409849325814 minutes\n",
      "53000 files processed in 28.57559439341227 minutes\n",
      "54000 files processed in 28.975828949610392 minutes\n",
      "55000 files processed in 29.35249685049057 minutes\n",
      "56000 files processed in 29.716948159535725 minutes\n",
      "57000 files processed in 30.069165909290312 minutes\n",
      "58000 files processed in 30.429383583863576 minutes\n",
      "59000 files processed in 30.78160128593445 minutes\n",
      "60000 files processed in 31.21116936604182 minutes\n",
      "61000 files processed in 31.570253841082256 minutes\n",
      "62000 files processed in 31.919788312911987 minutes\n",
      "63000 files processed in 32.78285765647888 minutes\n",
      "64000 files processed in 33.132525591055554 minutes\n",
      "65000 files processed in 33.47641005118688 minutes\n",
      "66000 files processed in 33.83146118323008 minutes\n",
      "67000 files processed in 34.18149562279383 minutes\n",
      "68000 files processed in 34.521329991022746 minutes\n",
      "69000 files processed in 34.858847725391385 minutes\n",
      "70000 files processed in 35.20988221963247 minutes\n",
      "71000 files processed in 35.55995010534922 minutes\n",
      "72000 files processed in 35.902267865339915 minutes\n",
      "73000 files processed in 36.24573563337326 minutes\n",
      "74000 files processed in 36.584086740016936 minutes\n",
      "75000 files processed in 36.93130448261897 minutes\n",
      "76000 files processed in 37.28512227535248 minutes\n",
      "77000 files processed in 37.62617336114248 minutes\n",
      "78000 files processed in 37.971741167704266 minutes\n",
      "79000 files processed in 38.30825895468394 minutes\n",
      "80000 files processed in 38.66114330689113 minutes\n",
      "81000 files processed in 39.024977854887645 minutes\n",
      "82000 files processed in 39.378328982989 minutes\n",
      "83000 files processed in 39.72926344474157 minutes\n",
      "84000 files processed in 40.07428113619486 minutes\n",
      "85000 files processed in 40.430548854668935 minutes\n",
      "86000 files processed in 40.790183305740356 minutes\n",
      "87000 files processed in 41.140634469191234 minutes\n",
      "88000 files processed in 41.48785212834676 minutes\n",
      "Error: could not parse header in logs/log.5145053ee4b0bef57ec85eb3.1393172624403.txt\n",
      "error in file log.5145053ee4b0bef57ec85eb3.1393172624403.txt\n",
      "89000 files processed in 41.86295334895452 minutes\n",
      "90000 files processed in 42.21148784160614 minutes\n",
      "91000 files processed in 42.55615568955739 minutes\n",
      "92000 files processed in 42.90207344690959 minutes\n",
      "93000 files processed in 43.26722450653712 minutes\n",
      "94000 files processed in 43.62572571436564 minutes\n",
      "95000 files processed in 43.98182676633199 minutes\n",
      "96000 files processed in 44.34351120392481 minutes\n",
      "97000 files processed in 44.709595735867815 minutes\n",
      "98000 files processed in 45.05416347980499 minutes\n",
      "99000 files processed in 45.39879781802495 minutes\n",
      "100000 files processed in 45.75001555283864 minutes\n",
      "101000 files processed in 46.08834993839264 minutes\n",
      "102000 files processed in 46.44290099541346 minutes\n",
      "103000 files processed in 46.799285519123075 minutes\n",
      "104000 files processed in 47.16738660732905 minutes\n",
      "105000 files processed in 47.52825437784195 minutes\n",
      "106000 files processed in 47.87927225828171 minutes\n",
      "107000 files processed in 48.239440027872725 minutes\n",
      "108000 files processed in 48.578957772254945 minutes\n",
      "109000 files processed in 48.92464217742284 minutes\n",
      "110000 files processed in 49.23214326699575 minutes\n",
      "111000 files processed in 49.709528080622356 minutes\n",
      "112000 files processed in 50.074095940589906 minutes\n",
      "113000 files processed in 50.547580913702646 minutes\n",
      "114000 files processed in 51.23299987316132 minutes\n",
      "115000 files processed in 51.5851676662763 minutes\n",
      "116000 files processed in 51.938002061843875 minutes\n",
      "117000 files processed in 52.28971981604894 minutes\n",
      "118000 files processed in 52.64467095534007 minutes\n",
      "119000 files processed in 52.987955351670585 minutes\n",
      "120000 files processed in 53.34280638694763 minutes\n",
      "121000 files processed in 53.689490830898286 minutes\n",
      "122000 files processed in 54.048479958375296 minutes\n",
      "123000 files processed in 54.393281042575836 minutes\n",
      "124000 files processed in 54.743732154369354 minutes\n",
      "125000 files processed in 55.105266551176705 minutes\n",
      "126000 files processed in 55.448950962225595 minutes\n",
      "127000 files processed in 55.81230214436849 minutes\n",
      "128000 files processed in 56.14913656314214 minutes\n",
      "129000 files processed in 56.4954042951266 minutes\n",
      "130000 files processed in 56.95610576073329 minutes\n",
      "131000 files processed in 57.498991008599596 minutes\n",
      "132000 files processed in 57.89614221652349 minutes\n",
      "133000 files processed in 58.25530992348989 minutes\n",
      "134000 files processed in 58.625394423802696 minutes\n",
      "135000 files processed in 58.99719559351603 minutes\n",
      "136000 files processed in 59.355796639124556 minutes\n",
      "137000 files processed in 59.71213108301163 minutes\n",
      "138000 files processed in 60.06861549218495 minutes\n",
      "139000 files processed in 60.321916274229686 minutes\n",
      "140000 files processed in 60.39361651738485 minutes\n"
     ]
    }
   ],
   "source": [
    "num_games, game_gains, total_gains = combine_logs()\n",
    "\n",
    "#Let's save pickles so I don't have to run this again.\n",
    "with open('gain_matrices.pkl', 'wb') as handle:\n",
    "    pickle.dump((num_games, game_gains, total_gains),handle)\n",
    "with open('card_list.pkl', 'wb') as handle:\n",
    "    pickle.dump((card_list,card_dict),handle)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
